This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: ./apps/backend/
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
</notes>

<additional_info>
<user_provided_header>
This repository contains the source code for the Repomix tool.
Repomix is designed to pack repository contents into a single file,
making it easier for AI systems to analyze and process the codebase.

Key Features:
- Configurable ignore patterns
- Custom header text support
- Efficient file processing and packing

Please refer to the README.md file for more detailed information on usage and configuration.

</user_provided_header>

</additional_info>

</file_summary>

<directory_structure>
apps/
  backend/
    app/
      agents/
        prompts/
          director.md
        roles.py
      infra/
        actions/
          chem.yaml
          docs.yaml
          rag.yaml
      mcp/
        client.py
        manager.py
      routes/
        agents.py
        evidence.py
        repo.py
        streams.py
        tasks.py
        workflows.py
      services/
        chem.py
        docs.py
        models.py
        rag.py
      tools/
        __init__.py
        local.py
      cli.py
      main.py
    Dockerfile
    pyproject.toml
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="apps/backend/app/agents/prompts/director.md">
You are DIRECTOR — a scientific research coordinator. Your job is to coordinate multiple specialist agents to produce original, evidence-based answers with explicit citations.

Operating principles
- Run a short plan: determine which specialists are needed and in what order.
- Maximize recall first, then precision. Prefer recent primary sources and systematic reviews.
- Be transparent about uncertainty and assumptions; clearly mark limitations.
- Never fabricate citations. Use [1]-style numeric citations and include sufficient bibliographic info.

Workflow (plan → gather → synthesize → critique → finalize)
1) Plan: Decide which specialists to call. By default, fan out SCOUT, SCHOLAR, ARCHIVIST in parallel using run_all_specialists_parallel.
2) Gather: If the topic is chemistry or design-heavy, call ALCHEMIST. Use CHEM outputs conservatively and include safety notes.
3) Synthesize: Call ANALYST to merge specialist outputs into a coherent answer with sections (Summary, Evidence, Gaps, Risks, References).
4) Critique: Check for missing perspectives, stale sources, and over-claims. If needed, re-call a specialist with a targeted follow-up.
5) Finalize: Produce a concise, well-structured response with explicit [1]-style citations and a short uncertainty note.

Tool usage rules
- scout: fast literature QA with citations; use for quick fact-finding and initial coverage.
- scholar: deep review; use for meta-analysis, methods, and critique.
- archivist: prior art/novelty and precedent checks; use for “has anyone done X?”
- alchemist: chemistry planning/design; use for candidate generation and constraints.
- analyst: final synthesis; always call before final answer.
- run_all_specialists_parallel: call early to gather breadth from scout, scholar, archivist concurrently, then refine.

Output contract
- Answer with sections: Summary, Evidence, Gaps, Risks, References.
- Citations must be [1]-style with DOI/URL and a short snippet.
- If confidence < medium, state why and what would reduce uncertainty.
</file>

<file path="apps/backend/app/agents/roles.py">
import os
import json
from typing import Any, Dict

import httpx
import time
from contextvars import ContextVar
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

try:
    from agents import Agent, Runner, function_tool, ModelSettings  # type: ignore
except Exception as _e:  # pragma: no cover
    Agent = None  # type: ignore
    Runner = None  # type: ignore
    ModelSettings = None  # type: ignore
    def function_tool(fn=None, **_kwargs):  # type: ignore
        def deco(f):
            return f
        return deco if fn is None else deco(fn)


INTERNAL_BEARER = os.getenv("ACTIONS_BEARER", "dev-token")
BASE_URL = os.getenv("RUNIX_BASE_URL", "http://localhost:8787")


async def _get_headers() -> Dict[str, str]:
    return {"Authorization": f"Bearer {INTERNAL_BEARER}"}


# Context var for capturing tool traces during a task
TOOL_TRACE_CVAR: ContextVar[list[dict] | None] = ContextVar("tool_trace", default=None)


@function_tool
@retry(
    reraise=True,
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=0.2, min=0.2, max=2),
    retry=retry_if_exception_type(httpx.HTTPError),
)
async def rag_search(q: str, k: int = 3) -> str:
    start = time.perf_counter()
    async with httpx.AsyncClient(base_url=BASE_URL, timeout=20) as client:
        resp = await client.get("/services/rag/search", params={"q": q, "k": k}, headers=await _get_headers())
        resp.raise_for_status()
        out = resp.json()
    elapsed = int((time.perf_counter() - start) * 1000)
    trace = TOOL_TRACE_CVAR.get()
    if trace is not None:
        trace.append({"tool": "rag.search", "args": {"q": q, "k": k}, "t_ms": elapsed})
    return json.dumps(out)


@function_tool
@retry(
    reraise=True,
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=0.2, min=0.2, max=2),
    retry=retry_if_exception_type(httpx.HTTPError),
)
async def rag_expand(q: str, n: int = 3) -> str:
    start = time.perf_counter()
    async with httpx.AsyncClient(base_url=BASE_URL, timeout=20) as client:
        resp = await client.post("/services/rag/expand", json={"q": q, "n": n}, headers=await _get_headers())
        resp.raise_for_status()
        out = resp.json()
    elapsed = int((time.perf_counter() - start) * 1000)
    trace = TOOL_TRACE_CVAR.get()
    if trace is not None:
        trace.append({"tool": "rag.expand", "args": {"q": q, "n": n}, "t_ms": elapsed})
    return json.dumps(out)


@function_tool
def format_markdown_with_refs(blocks_json: str) -> str:
    from app.tools.local import format_markdown_with_refs as _fmt
    try:
        blocks = json.loads(blocks_json)
    except Exception:
        blocks = []
    return _fmt(blocks)


@function_tool
def chem_calc(smiles: str) -> str:
    from app.tools.local import chem_calc as _chem
    return json.dumps(_chem(smiles))


@function_tool
@retry(
    reraise=True,
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=0.2, min=0.2, max=2),
    retry=retry_if_exception_type(Exception),
)
async def web_search_openai(q: str, max_results: int = 5) -> str:
    # Uses OpenAI Responses API built-in Web Search tool
    start = time.perf_counter()
    api_key = os.getenv("OPENAI_API_KEY", "")
    fake = os.getenv("RUNIX_FAKE_OAI", "") == "1"
    if not api_key and not fake:
        return json.dumps({"error": "Missing OPENAI_API_KEY"})
    try:
        from openai import OpenAI  # lazy import
        client = OpenAI(api_key=api_key) if api_key else OpenAI()
        resp = client.responses.create(
            model=os.getenv("MODEL_WEB_SEARCH", "gpt-4o-mini"),
            input=q,
            tools=[{"type": "web_search"}],
            tool_choice="auto",
            max_output_tokens=1000,
            extra_body={"web_search": {"max_results": max_results}},
        )
        text = getattr(resp, "output_text", "") or ""
        out = {"text": text}
    except Exception as e:
        out = {"error": str(e)}
    elapsed = int((time.perf_counter() - start) * 1000)
    trace = TOOL_TRACE_CVAR.get()
    if trace is not None:
        trace.append({"tool": "web.openai_search", "args": {"q": q[:200], "max_results": max_results}, "t_ms": elapsed})
    return json.dumps(out)

@function_tool
@retry(
    reraise=True,
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=0.2, min=0.2, max=2),
    retry=retry_if_exception_type(httpx.HTTPError),
)
async def chem_design(constraints_json: str | None = None, n: int = 3) -> str:
    start = time.perf_counter()
    async with httpx.AsyncClient(base_url=BASE_URL, timeout=20) as client:
        constraints: Dict[str, Any] = {}
        if constraints_json:
            try:
                constraints = json.loads(constraints_json)
            except Exception:
                constraints = {}
        resp = await client.post("/services/chem/design", json={"constraints": constraints, "n": n}, headers=await _get_headers())
        resp.raise_for_status()
        out = resp.json()
    elapsed = int((time.perf_counter() - start) * 1000)
    trace = TOOL_TRACE_CVAR.get()
    if trace is not None:
        trace.append({"tool": "chem.design", "args": {"n": n}, "t_ms": elapsed})
    return json.dumps(out)


def _scout_instructions() -> str:
    return (
        "You are SCOUT, a fast literature QA agent. Answer succinctly with [1]-style citations. "
        "Prefer recent primary sources. If confidence is low, state it clearly."
    )


def _scholar_instructions() -> str:
    return (
        "You are SCHOLAR, a deep review agent. Follow: plan → gather → synthesize → critique → finalize. "
        "Answer with citations and a short critique section."
    )


def _archivist_instructions() -> str:
    return (
        "You are ARCHIVIST, a precedent search agent. Focus on prior art and 'has anyone done X'. "
        "Emphasize novelty assessment and cite sources."
    )


def _alchemist_instructions() -> str:
    return (
        "You are ALCHEMIST, a chemistry planning agent. Use chem tools. "
        "Output a table of candidates, reasoning, and a safety disclaimer."
    )


def _analyst_instructions() -> str:
    return (
        "You are ANALYST, an analysis agent. For Phase 0, provide a structured template with placeholders."
    )


def _critic_instructions() -> str:
    return (
        "You are CRITIC, a verification agent. Cross-check claims for contradictions or overreach. "
        "Flag weak evidence and suggest additional queries. Output flags + suggestions."
    )


ALIASES = {
    # legacy → new canonical
    "CROW": "SCOUT",
    "FALCON": "SCHOLAR",
    "OWL": "ARCHIVIST",
    "PHOENIX": "ALCHEMIST",
    "FINCH": "ANALYST",
    "AUTO": "DIRECTOR",
}


def normalize_agent_name(name: str) -> str:
    key = (name or "").strip().upper()
    return ALIASES.get(key, key)


def build_agents(mcp_servers: list[Any] | None = None) -> dict[str, Any]:
    if Agent is None:
        raise ImportError("openai-agents is not installed")

    common_settings = ModelSettings(parallel_tool_calls=True) if ModelSettings else None

    scout = Agent(
        name="SCOUT",
        instructions=_scout_instructions(),
        tools=[rag_search, rag_expand, format_markdown_with_refs],
        model=os.getenv("MODEL_SCOUT", "gpt-4o-mini"),
        mcp_servers=mcp_servers or [],
        model_settings=common_settings,
    )

    scholar = Agent(
        name="SCHOLAR",
        instructions=_scholar_instructions(),
        tools=[rag_search, rag_expand, format_markdown_with_refs, web_search_openai],
        model=os.getenv("MODEL_SCHOLAR", "gpt-4o"),
        mcp_servers=mcp_servers or [],
        model_settings=common_settings,
    )

    archivist = Agent(
        name="ARCHIVIST",
        instructions=_archivist_instructions(),
        tools=[rag_search, format_markdown_with_refs, web_search_openai],
        model=os.getenv("MODEL_ARCHIVIST", "gpt-4o-mini"),
        mcp_servers=mcp_servers or [],
        model_settings=common_settings,
    )

    alchemist = Agent(
        name="ALCHEMIST",
        instructions=_alchemist_instructions(),
        tools=[chem_design, chem_calc],
        model=os.getenv("MODEL_ALCHEMIST", "gpt-4o-mini"),
        mcp_servers=mcp_servers or [],
        model_settings=common_settings,
    )

    analyst = Agent(
        name="ANALYST",
        instructions=_analyst_instructions(),
        tools=[],
        model=os.getenv("MODEL_ANALYST", "gpt-4o-mini"),
        mcp_servers=mcp_servers or [],
        model_settings=common_settings,
    )

    critic = Agent(
        name="CRITIC",
        instructions=_critic_instructions(),
        tools=[rag_search, rag_expand, web_search_openai],
        model=os.getenv("MODEL_CRITIC", "gpt-4o-mini"),
        mcp_servers=mcp_servers or [],
        model_settings=common_settings,
    )

    # Wrap specialists as tools for a Director orchestrator
    def _make_agent_tool(agent_obj: Any, tool_name: str, description: str):
        @function_tool(name_override=tool_name, description_override=description)
        async def _tool(input: str) -> str:
            # record start
            trace = TOOL_TRACE_CVAR.get()
            if trace is not None:
                trace.append({"tool": f"agent.{tool_name}", "args": {"input": input[:120]}, "phase": "start"})
            result = await Runner.run(agent_obj, input, max_turns=16)
            # record end
            if trace is not None:
                trace.append({"tool": f"agent.{tool_name}", "phase": "end"})
            return getattr(result, "final_output", "")
        return _tool

    scout_tool = _make_agent_tool(scout, "scout", "Fast literature QA with citations")
    scholar_tool = _make_agent_tool(scholar, "scholar", "Deep review with plan→gather→synthesize→critique→finalize")
    archivist_tool = _make_agent_tool(archivist, "archivist", "Precedent/novelty search")
    alchemist_tool = _make_agent_tool(alchemist, "alchemist", "Chem design and planning")
    analyst_tool = _make_agent_tool(analyst, "analyst", "Analysis template/synthesis")

    @function_tool(name_override="run_all_specialists_parallel", description_override="Run SCOUT, SCHOLAR, ARCHIVIST in parallel and return a dict of results (as JSON string)")
    async def run_all_specialists_parallel(input: str) -> str:
        # Parallel fan-out using asyncio.gather via Runner
        import asyncio
        results = await asyncio.gather(
            Runner.run(scout, input, max_turns=8),
            Runner.run(scholar, input, max_turns=16),
            Runner.run(archivist, input, max_turns=8),
        )
        names = ["scout", "scholar", "archivist"]
        out: Dict[str, str] = {}
        for name, res in zip(names, results):
            out[name] = getattr(res, "final_output", "")
        return json.dumps(out)

    # Load director prompt if available
    director_prompt_path = os.path.join(os.path.dirname(__file__), "prompts", "director.md")
    director_prompt = _alchemist_instructions()
    try:
        if os.path.exists(director_prompt_path):
            with open(director_prompt_path, "r", encoding="utf-8") as f:
                director_prompt = f.read()
    except Exception:
        pass

    director = Agent(
        name="DIRECTOR",
        instructions=director_prompt,
        tools=[
            scout_tool,
            scholar_tool,
            archivist_tool,
            alchemist_tool,
            analyst_tool,
            run_all_specialists_parallel,
        ],
        mcp_servers=mcp_servers or [],
        model=os.getenv("MODEL_DIRECTOR", "gpt-4o"),
        model_settings=ModelSettings(parallel_tool_calls=True, tool_choice="required") if ModelSettings else None,
    )

    return {
        "SCOUT": scout,
        "SCHOLAR": scholar,
        "ARCHIVIST": archivist,
        "ALCHEMIST": alchemist,
        "ANALYST": analyst,
        "CRITIC": critic,
        "DIRECTOR": director,
    }


_AGENTS_CACHE: dict[str, Any] | None = None


def get_agent(name: str):
    global _AGENTS_CACHE
    if _AGENTS_CACHE is None:
        _AGENTS_CACHE = build_agents()
    return _AGENTS_CACHE.get(normalize_agent_name(name))
</file>

<file path="apps/backend/app/infra/actions/chem.yaml">
openapi: 3.0.3
info:
  title: Chemistry Service (Stub)
  version: 0.1.0
servers:
  - url: http://localhost:8787/services
paths:
  /chem/design:
    post:
      summary: Propose candidate molecules
      security:
        - bearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                constraints:
                  type: object
                n:
                  type: integer
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  candidates:
                    type: array
                    items:
                      type: object
                      properties:
                        smiles:
                          type: string
                        score:
                          type: number
                        rationale:
                          type: string
                  disclaimer:
                    type: string
components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
</file>

<file path="apps/backend/app/infra/actions/docs.yaml">
openapi: 3.0.3
info:
  title: Docs Service (Stub)
  version: 0.1.0
servers:
  - url: http://localhost:8787/services
paths:
  /docs/ingest:
    post:
      summary: Ingest a document by DOI or URL
      security:
        - bearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                doi:
                  type: string
                url:
                  type: string
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  docId:
                    type: string
                  status:
                    type: string
                  note:
                    type: string
components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
</file>

<file path="apps/backend/app/infra/actions/rag.yaml">
openapi: 3.0.3
info:
  title: RAG Service (Stub)
  version: 0.1.0
servers:
  - url: http://localhost:8787/services
paths:
  /rag/search:
    get:
      summary: Search retrieval passages
      security:
        - bearerAuth: []
      parameters:
        - in: query
          name: q
          schema:
            type: string
          required: true
        - in: query
          name: k
          schema:
            type: integer
          required: false
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  passages:
                    type: array
                    items:
                      type: object
                      properties:
                        source:
                          type: string
                        text:
                          type: string
                        span:
                          type: array
                          items:
                            type: integer
  /rag/expand:
    post:
      summary: Query expansion
      security:
        - bearerAuth: []
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              properties:
                q:
                  type: string
                n:
                  type: integer
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                type: object
                properties:
                  queries:
                    type: array
                    items:
                      type: string
components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      bearerFormat: JWT
</file>

<file path="apps/backend/app/mcp/client.py">
import asyncio
import json
import os
from typing import Any, Dict, List, Optional

try:
    # Prefer modern API
    from mcp.transport.stdio import stdio_client, StdioServerParameters  # type: ignore
except Exception:  # pragma: no cover
    StdioServerParameters = None  # type: ignore
    try:
        from mcp.client.stdio import stdio_client, StdioServerParameters  # type: ignore
    except Exception:
        stdio_client = None  # type: ignore
        StdioServerParameters = None  # type: ignore

try:
    from mcp import ClientSession  # type: ignore
except Exception:  # pragma: no cover
    ClientSession = None  # type: ignore


MCP_DEFAULT_SERVERS = json.loads(os.getenv("MCP_SERVERS", "{}"))
# Example env format:
# MCP_SERVERS='{"web-fetch":{"command":"uvx","args":["mcp-science","web-fetch"]}}'


def _connect(server_name: str, server_cfg: Dict[str, Any]):
    if stdio_client is None:
        raise RuntimeError("mcp not installed. pip install mcp")
    if StdioServerParameters is None or ClientSession is None:
        raise RuntimeError("mcp client APIs unavailable; update mcp package")
    command = server_cfg.get("command")
    args = server_cfg.get("args", [])
    if not command:
        raise ValueError("Missing command for MCP server")
    params = StdioServerParameters(command=command, args=args)  # type: ignore
    return stdio_client(params)


async def list_tools(server_name: str, server_cfg: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
    cfg = server_cfg or MCP_DEFAULT_SERVERS.get(server_name)
    if not cfg:
        return []
    async with _connect(server_name, cfg) as streams:
        async with ClientSession(*streams) as session:  # type: ignore[misc]
            await session.initialize()
            resp = await session.list_tools()
            tools = getattr(resp, "tools", [])
            out: List[Dict[str, Any]] = []
            for t in tools:
                try:
                    out.append(t.model_dump())  # type: ignore[attr-defined]
                except Exception:
                    try:
                        out.append(json.loads(json.dumps(t)))
                    except Exception:
                        out.append({"name": str(getattr(t, "name", "unknown"))})
            return out


async def call_tool(server_name: str, tool_name: str, arguments: Dict[str, Any], server_cfg: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    cfg = server_cfg or MCP_DEFAULT_SERVERS.get(server_name)
    if not cfg:
        raise ValueError("Unknown MCP server; configure MCP_SERVERS env")
    async with _connect(server_name, cfg) as streams:
        async with ClientSession(*streams) as session:  # type: ignore[misc]
            await session.initialize()
            result = await session.call_tool(tool_name, arguments)
            # Normalize result object to plain dict
            try:
                return result.model_dump()  # type: ignore[attr-defined]
            except Exception:
                return json.loads(json.dumps(result, default=lambda o: getattr(o, "model_dump", lambda: str(o))()))


def _sync(coro):
    return asyncio.get_event_loop().run_until_complete(coro)


# Expose agent tools
try:
    from agents import function_tool  # type: ignore
except Exception:  # pragma: no cover
    def function_tool(fn=None, **_kwargs):  # type: ignore
        def deco(f):
            return f
        return deco if fn is None else deco(fn)


@function_tool(name_override="mcp_invoke", description_override="Invoke an MCP tool on a configured server. Args: server, tool, args_json")
async def mcp_invoke_tool(server: str, tool: str, args_json: str) -> str:
    args: Dict[str, Any] = {}
    try:
        args = json.loads(args_json) if args_json else {}
    except Exception:
        args = {}
    out = await call_tool(server, tool, args)
    return json.dumps(out)


@function_tool(name_override="mcp_web_fetch", description_override="Fetch a URL via MCP web-fetch server. Args: url")
async def mcp_web_fetch(url: str) -> str:
    server = "web-fetch"
    args = {"url": url}
    out = await call_tool(server, "web_fetch", args)
    return json.dumps(out)
</file>

<file path="apps/backend/app/mcp/manager.py">
import json
import os
from typing import Any, Dict, List

try:
    from agents.mcp import MCPServerStdioParams, MCPServerStdio, MCPServerStreamableHttp, MCPServerStreamableHttpParams  # type: ignore
except Exception:  # pragma: no cover
    MCPServerStdioParams = None  # type: ignore
    MCPServerStdio = None  # type: ignore
    MCPServerStreamableHttp = None  # type: ignore
    MCPServerStreamableHttpParams = None  # type: ignore


def _load_config() -> Dict[str, Any]:
    env = os.getenv("MCP_SERVERS", "{}")
    try:
        return json.loads(env) if env else {}
    except Exception:
        return {}


def build_mcp_servers() -> List[Any]:
    cfg = _load_config().get("mcpServers") or _load_config()
    servers: List[Any] = []
    if not cfg:
        return servers
    for name, spec in cfg.items():
        # Two styles supported:
        # 1) stdio: {"command": "uvx", "args": ["mcp-science","web-fetch"]}
        # 2) http/streamable: {"url": "https://...", "headers": {...}}
        if "url" in spec and MCPServerStreamableHttp and MCPServerStreamableHttpParams:
            params = MCPServerStreamableHttpParams(
                url=spec["url"],
                headers=spec.get("headers", {}),
                timeout=float(spec.get("timeout", 60.0)),
            )
            servers.append(
                MCPServerStreamableHttp(
                    params,
                    name=name,
                    client_session_timeout_seconds=int(spec.get("client_session_timeout_seconds", 120)),
                )
            )
        elif "command" in spec and MCPServerStdio and MCPServerStdioParams:
            params = MCPServerStdioParams(
                command=spec["command"],
                args=spec.get("args", []),
                env=spec.get("env", {}),
                cwd=spec.get("cwd"),
            )
            servers.append(
                MCPServerStdio(
                    params,
                    name=name,
                    client_session_timeout_seconds=int(spec.get("client_session_timeout_seconds", 120)),
                )
            )
    return servers
</file>

<file path="apps/backend/app/routes/agents.py">
from fastapi import APIRouter
from app.agents.roles import build_agents
from typing import Any

router = APIRouter()


@router.get("/v1/agents")
def list_agents():
    try:
        agents = build_agents()
        return {"agents": sorted(list(agents.keys()))}
    except Exception:
        # Fallback to canonical names if Agents SDK isn't installed
        return {"agents": ["SCOUT", "SCHOLAR", "ARCHIVIST", "ALCHEMIST", "ANALYST", "DIRECTOR"]}


@router.get("/v1/agents/aliases")
def list_aliases():
    from app.agents.roles import ALIASES  # lazy import to avoid cycles
    return {"aliases": ALIASES}


@router.get("/v1/mcp/tools")
async def list_mcp_tools(server: str):
    try:
        from app.mcp.manager import build_mcp_servers  # type: ignore
        servers = build_mcp_servers()
        # Use Agents SDK server object to list tools
        target = None
        for s in servers:
            if getattr(s, "name", "") == server:
                target = s
                break
        if target is None:
            return {"server": server, "error": "not configured"}
        await target.connect()
        tools_list = await target.list_tools()
        # Normalize
        tools = [getattr(t, "model_dump", lambda: {"name": getattr(t, "name", "unknown")})() for t in tools_list]
        await target.cleanup()
        return {"server": server, "tools": tools}
    except Exception as e:  # pragma: no cover
        return {"server": server, "error": str(e)}
</file>

<file path="apps/backend/app/routes/evidence.py">
from fastapi import APIRouter, Request
from fastapi.responses import JSONResponse
import os
import sqlite3

router = APIRouter()

DB_PATH = os.getenv("RUNIX_TASKS_DB", os.path.join(os.path.dirname(__file__), "..", "tasks.db"))


def _connect():
    path = os.path.abspath(DB_PATH)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    conn = sqlite3.connect(path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn


@router.get("/v1/evidence")
async def list_evidence(task_id: str | None = None):
    conn = _connect()
    cur = conn.cursor()
    if task_id:
        cur.execute("SELECT * FROM evidence WHERE task_id=? ORDER BY id", (task_id,))
    else:
        cur.execute("SELECT * FROM evidence ORDER BY id DESC LIMIT 200")
    rows = [dict(r) for r in cur.fetchall()]
    conn.close()
    return {"evidence": rows}
</file>

<file path="apps/backend/app/routes/repo.py">
import os
from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel
from typing import List, Literal, Optional
import subprocess
import shlex

router = APIRouter()

# Resolve monorepo root: apps/backend/app/routes -> ../../../../ (repo root)
BST_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
REPOS = {
    "demo": os.path.join(BST_ROOT, 'examples', 'hello-world-trial')
}


class TreeNode(BaseModel):
    name: str
    path: str
    type: Literal['file', 'dir']
    children: Optional[List['TreeNode']] = None


def _is_allowed(base: str, target: str) -> bool:
    rel = os.path.relpath(target, base)
    return rel and not rel.startswith('..') and not os.path.isabs(rel)


def _build_tree(dir_path: str, base: str, depth: int = 0, max_depth: int = 4) -> List[TreeNode]:
    if depth > max_depth:
        return []
    nodes: List[TreeNode] = []
    try:
        for name in sorted(os.listdir(dir_path)):
            if name.startswith('.') or name in ('node_modules', 'dist', 'build'):
                continue
            full = os.path.join(dir_path, name)
            if not _is_allowed(base, full):
                continue
            if os.path.isdir(full):
                nodes.append(TreeNode(name=name, path=os.path.relpath(full, base), type='dir', children=_build_tree(full, base, depth+1, max_depth)))
            else:
                nodes.append(TreeNode(name=name, path=os.path.relpath(full, base), type='file'))
    except Exception:
        pass
    # sort with dirs first
    nodes.sort(key=lambda n: (0 if n.type == 'dir' else 1, n.name))
    return nodes


@router.get('/repo/tree')
def get_repo_tree(repo: str = Query('demo')):
    base = REPOS.get(repo)
    if not base:
        raise HTTPException(404, 'Unknown repo')
    if not os.path.exists(base):
        raise HTTPException(404, 'Repo path missing')
    return {"repo": repo, "root": os.path.basename(base), "tree": [t.model_dump() for t in _build_tree(base, base)]}


class FileWrite(BaseModel):
    repo: str
    path: str
    content: str


@router.get('/repo/file')
def read_file(repo: str = Query('demo'), path_q: str = Query('')):
    base = REPOS.get(repo)
    if not base or not path_q:
        raise HTTPException(400, 'Missing')
    target = os.path.join(base, path_q)
    rel = os.path.relpath(target, base)
    if rel.startswith('..'):
        raise HTTPException(400, 'Invalid path')
    try:
        with open(target, 'r', encoding='utf-8') as f:
            return {"repo": repo, "path": path_q, "content": f.read()}
    except Exception as e:
        raise HTTPException(500, 'Failed to read file')


@router.post('/repo/file')
def write_file(body: FileWrite):
    base = REPOS.get(body.repo)
    if not base or not body.path:
        raise HTTPException(400, 'Missing')
    target = os.path.join(base, body.path)
    rel = os.path.relpath(target, base)
    if rel.startswith('..'):
        raise HTTPException(400, 'Invalid path')
    os.makedirs(os.path.dirname(target), exist_ok=True)
    with open(target, 'w', encoding='utf-8') as f:
        f.write(body.content)
    return {"ok": True}


@router.get('/repo/branches')
def get_branches(repo: str = Query('demo')):
    # Demo: return a static set
    return {"repo": repo, "branches": ["main", "dev", "experiment/analysis"]}


@router.get('/repo/search')
def search_repo(repo: str = Query('demo'), q: str = Query('')):
    base = REPOS.get(repo)
    if not base:
        raise HTTPException(404, 'Unknown repo')
    if not q:
        return {"results": []}
    results = []
    ql = q.lower()
    for root, dirs, files in os.walk(base):
        # skip hidden / heavy dirs
        dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ('node_modules', 'dist', 'build')]
        for f in files:
            if f.startswith('.'):
                continue
            p = os.path.join(root, f)
            rel = os.path.relpath(p, base)
            # filename match
            if ql in f.lower():
                results.append({"path": rel, "preview": f"{f}"})
                continue
            try:
                # small text files only
                if os.path.getsize(p) <= 200_000:
                    with open(p, 'r', encoding='utf-8', errors='ignore') as fh:
                        for i, line in enumerate(fh.readlines()[:500]):
                            if ql in line.lower():
                                results.append({"path": rel, "line": i+1, "preview": line.strip()[:200]})
                                break
            except Exception:
                pass
            if len(results) >= 50:
                break
        if len(results) >= 50:
            break
    return {"results": results}


class ExecRequest(BaseModel):
    repo: str
    cmd: str
    cwd: Optional[str] = None  # relative to repo root


ALLOW_CMDS = {"ls", "cat", "head", "tail", "wc", "pwd", "echo", "python", "python3", "mkdir", "touch"}


@router.post('/repo/exec')
def exec_repo(body: ExecRequest):
    base = REPOS.get(body.repo)
    if not base:
        raise HTTPException(404, 'Unknown repo')
    cmd = body.cmd.strip()
    if not cmd:
        return {"ok": True, "code": 0, "stdout": "", "stderr": ""}
    try:
        parts = shlex.split(cmd)
        if parts[0] not in ALLOW_CMDS:
            raise HTTPException(400, f"Command not allowed: {parts[0]}")
        # Working directory under repo root
        cwd = base
        if body.cwd:
            rel = os.path.normpath(body.cwd).replace('\\', '/')
            if rel.startswith('..') or rel.startswith('/'):
                raise HTTPException(400, 'Invalid cwd')
            cwd = os.path.join(base, rel)
        # Validate args: reject absolute/parent paths
        for p in parts[1:]:
            if p.startswith('/') or '..' in p:
                raise HTTPException(400, 'Invalid path argument')
        # Restrict python to run_demo.py only
        if parts[0] in ("python", "python3"):
            if len(parts) < 2 or not parts[1].endswith('run_demo.py'):
                raise HTTPException(400, 'Only run_demo.py is allowed')
        res = subprocess.run(cmd, cwd=cwd, shell=True, capture_output=True, text=True, timeout=15)
        return {"ok": True, "code": res.returncode, "stdout": res.stdout, "stderr": res.stderr}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, 'Exec failed')
</file>

<file path="apps/backend/app/routes/streams.py">
from fastapi import APIRouter
from fastapi.responses import StreamingResponse
import os
import sqlite3
import time
import json

router = APIRouter()

DB_PATH = os.getenv("RUNIX_TASKS_DB", os.path.join(os.path.dirname(__file__), "..", "tasks.db"))


def _connect():
    path = os.path.abspath(DB_PATH)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    conn = sqlite3.connect(path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn


@router.get("/v1/streams/tasks/{task_id}")
async def stream_task(task_id: str):
    def gen():
        yield "event: open\n\n"
        conn = _connect()
        cur = conn.cursor()
        cur.execute("SELECT author, content, created_at FROM messages WHERE task_id=? ORDER BY id", (task_id,))
        rows = cur.fetchall()
        for r in rows:
            payload = {"message": {"author": r["author"], "content": r["content"], "at": r["created_at"]}}
            yield "data: " + json.dumps(payload) + "\n\n"
        conn.close()
        yield "data: " + json.dumps({"done": True, "task_id": task_id}) + "\n\n"
    headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
    return StreamingResponse(gen(), media_type="text/event-stream", headers=headers)
</file>

<file path="apps/backend/app/routes/tasks.py">
import os
import sqlite3
import uuid
import json
import time
from datetime import datetime
from typing import Optional

import asyncio
from fastapi import APIRouter, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from openai import OpenAI
from app.tools.local import extract_citations
from app.agents.roles import TOOL_TRACE_CVAR
import hashlib

DB_PATH = os.getenv("RUNIX_TASKS_DB", os.path.join(os.path.dirname(__file__), "..", "tasks.db"))


def _connect():
    path = os.path.abspath(DB_PATH)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    conn = sqlite3.connect(path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn


def _init_db():
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS tasks (
            id TEXT PRIMARY KEY,
            agent TEXT NOT NULL,
            status TEXT NOT NULL,
            created_at TEXT NOT NULL,
            updated_at TEXT NOT NULL,
            thread_id TEXT,
            run_id TEXT,
            answer_markdown TEXT
        )
        """
    )
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS messages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_id TEXT NOT NULL,
            author TEXT NOT NULL,
            content TEXT NOT NULL,
            created_at TEXT NOT NULL
        )
        """
    )
    # Evidence storage for passages/DOIs used per task
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS evidence (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            task_id TEXT NOT NULL,
            doc_id TEXT,
            source_type TEXT,
            section TEXT,
            span_start INTEGER,
            span_end INTEGER,
            text_hash TEXT,
            figure_id TEXT,
            table_id TEXT,
            claim_id TEXT,
            raw_text TEXT,
            created_at TEXT NOT NULL
        )
        """
    )
    # Migration: add citation_index if missing
    cur.execute("PRAGMA table_info(evidence)")
    cols = [r[1] for r in cur.fetchall()]
    if "citation_index" not in cols:
        try:
            cur.execute("ALTER TABLE evidence ADD COLUMN citation_index INTEGER")
        except Exception:
            pass
    conn.commit()
    conn.close()


_init_db()


def _now_iso() -> str:
    return datetime.utcnow().isoformat() + "Z"


def _insert_evidence_for_task(task_id: str, answer_text: str | None):
    if not answer_text:
        return
    try:
        cits = extract_citations(answer_text) or []
    except Exception:
        cits = []
    if not cits:
        return
    conn = _connect()
    cur = conn.cursor()
    now = _now_iso()
    for c in cits:
        doc_id = c.get("doi") or c.get("url") or c.get("title") or "unknown"
        source_type = "doi" if c.get("doi") else ("url" if c.get("url") else "unknown")
        section = "unknown"
        snippet = c.get("snippet") or ""
        text_hash = hashlib.sha256(snippet.encode("utf-8")).hexdigest() if snippet else None
        citation_index = c.get("index") if isinstance(c.get("index"), int) else None
        cur.execute(
            """
            INSERT INTO evidence (task_id, doc_id, source_type, section, span_start, span_end, text_hash, figure_id, table_id, claim_id, raw_text, created_at, citation_index)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
            (task_id, doc_id, source_type, section, 0, 0, text_hash, None, None, None, snippet, now, citation_index),
        )
    conn.commit()
    conn.close()


class CreateTaskRequest(BaseModel):
    agent: str
    query: str
    stream: Optional[bool] = False


router = APIRouter()

# Simple in-memory rate limiter (Phase 0)
_RATE_LIMIT: dict[str, list[float]] = {}
_RATE_LIMIT_MAX = int(os.getenv("RUNIX_RATE_LIMIT", "30"))
_RATE_WINDOW_S = 60.0


@router.post("/v1/tasks")
async def create_task(req: Request):
    body = await req.json()
    try:
        payload = CreateTaskRequest(**body)
    except Exception:
        return JSONResponse({"error": "Invalid request"}, status_code=400)

    # Rate limit per IP
    ip = (req.client.host if req.client else "unknown")
    now = time.time()
    buf = _RATE_LIMIT.get(ip, [])
    buf = [t for t in buf if now - t < _RATE_WINDOW_S]
    if len(buf) >= _RATE_LIMIT_MAX:
        return JSONResponse({"error": "Rate limit exceeded"}, status_code=429)
    buf.append(now)
    _RATE_LIMIT[ip] = buf

    auth = req.headers.get("authorization") or ""
    bearer = auth.split(" ", 1)[1] if auth.lower().startswith("bearer ") and len(auth.split(" ", 1)) > 1 else None
    api_key = bearer or os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        return JSONResponse({"error": "Missing OpenAI API key"}, status_code=401)

    task_id = str(uuid.uuid4())
    # Size limits (Phase 0): reject overly long queries
    if len(payload.query or "") > 8000:
        return JSONResponse({"error": "Query too long"}, status_code=413)
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO tasks (id, agent, status, created_at, updated_at) VALUES (?, ?, ?, ?, ?)",
        (task_id, payload.agent.upper(), "running", _now_iso(), _now_iso()),
    )
    cur.execute(
        "INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)",
        (task_id, "User", payload.query, _now_iso()),
    )
    conn.commit()
    conn.close()

    client = OpenAI(api_key=api_key)

    if payload.stream and (payload.agent or "").strip().upper() in {"DIRECTOR", "AUTO"}:
        # Multi-step streaming using Agents SDK orchestration for DIRECTOR
        async def agen():
            yield "event: open\n\n"
            TOOL_TRACE_CVAR.set([])
            from app.agents.roles import build_agents  # type: ignore
            try:
                from agents import Runner  # type: ignore
            except Exception:
                yield "data: " + json.dumps({"error": "Agents SDK not installed. pip install openai-agents"}) + "\n\n"
                return

            try:
                # Build MCP servers if configured and pass to DIRECTOR
                mcp_servers = []
                try:
                    from app.mcp.manager import build_mcp_servers  # type: ignore
                    mcp_servers = build_mcp_servers()
                    # Connect servers up-front for faster first call
                    for s in mcp_servers:
                        try:
                            await s.connect()
                        except Exception:
                            pass
                except Exception:
                    mcp_servers = []

                agents_map = build_agents(mcp_servers=mcp_servers)
                scout = agents_map["SCOUT"]
                scholar = agents_map["SCHOLAR"]
                archivist = agents_map["ARCHIVIST"]
                alchemist = agents_map["ALCHEMIST"]
                analyst = agents_map["ANALYST"]

                input_text = payload.query

                # Announce tool calls and run in parallel
                for name in ("scout", "scholar", "archivist"):
                    yield "data: " + json.dumps({"event": "tool_call", "tool": f"agent.{name}"}) + "\n\n"
                scout_res, scholar_res, archivist_res = await asyncio.gather(
                    Runner.run(scout, input_text, max_turns=8),
                    Runner.run(scholar, input_text, max_turns=16),
                    Runner.run(archivist, input_text, max_turns=8),
                )
                for name in ("scout", "scholar", "archivist"):
                    yield "data: " + json.dumps({"event": "tool_result", "tool": f"agent.{name}"}) + "\n\n"

                results = {
                    "scout": getattr(scout_res, "final_output", ""),
                    "scholar": getattr(scholar_res, "final_output", ""),
                    "archivist": getattr(archivist_res, "final_output", ""),
                }

                # Optional chemistry step
                if any(k in input_text.lower() for k in ["smiles", "molecule", "kinase", "chem"]):
                    yield "data: " + json.dumps({"event": "tool_call", "tool": "agent.alchemist"}) + "\n\n"
                    chem_res = await Runner.run(alchemist, input_text, max_turns=8)
                    yield "data: " + json.dumps({"event": "tool_result", "tool": "agent.alchemist"}) + "\n\n"
                    results["alchemist"] = getattr(chem_res, "final_output", "")

                # Synthesis via analyst
                yield "data: " + json.dumps({"event": "tool_call", "tool": "agent.analyst"}) + "\n\n"
                synth_input = f"Synthesize these findings with citations: {json.dumps(results)[:4000]}"
                synth = await Runner.run(analyst, synth_input, max_turns=8)
                final_text = getattr(synth, "final_output", "")
                yield "data: " + json.dumps({"event": "tool_result", "tool": "agent.analyst"}) + "\n\n"

                # Persist and emit final
                conn2 = _connect()
                cur2 = conn2.cursor()
                cur2.execute(
                    "UPDATE tasks SET status=?, answer_markdown=?, updated_at=? WHERE id=?",
                    ("succeeded", final_text, _now_iso(), task_id),
                )
                cur2.execute(
                    "INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)",
                    (task_id, "AI", final_text, _now_iso()),
                )
                conn2.commit()
                conn2.close()

                yield "data: " + json.dumps({
                    "done": True,
                    "task_id": task_id,
                    "message": {"id": 0, "author": "AI", "content": final_text},
                }) + "\n\n"
                _insert_evidence_for_task(task_id, final_text)
            except Exception as e:
                yield "data: " + json.dumps({"error": str(e)}) + "\n\n"
                return

        headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
        return StreamingResponse(agen(), media_type="text/event-stream", headers=headers)

    if payload.stream:
        def gen():
            from app.main import build_system_prompt  # lazy import

            buffer = ""
            yield "event: open\n\n"
            instructions = build_system_prompt(payload.agent)
            with client.responses.stream(
                model="gpt-4o-mini",
                instructions=instructions,
                input=f"User: {payload.query}\nAssistant:",
                temperature=0.2,
                max_output_tokens=1000,
            ) as stream:
                for event in stream:
                    etype = getattr(event, "type", "")
                    if etype == "response.output_text.delta":
                        delta = getattr(event, "delta", "") or ""
                        if delta:
                            buffer += delta
                            yield "data: " + json.dumps({"delta": delta}) + "\n\n"
                final_resp = stream.get_final_response()
                text = getattr(final_resp, "output_text", None) or buffer

            # persist final
            conn2 = _connect()
            cur2 = conn2.cursor()
            cur2.execute(
                "UPDATE tasks SET status=?, answer_markdown=?, updated_at=? WHERE id=?",
                ("succeeded", text, _now_iso(), task_id),
            )
            cur2.execute(
                "INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)",
                (task_id, "AI", text, _now_iso()),
            )
            conn2.commit()
            conn2.close()

            # Persist evidence rows from extracted citations
            _insert_evidence_for_task(task_id, text)

            final = {
                "done": True,
                "task_id": task_id,
                "message": {"id": 0, "author": "AI", "content": text},
                "citations": extract_citations(text or ""),
            }
            yield "data: " + json.dumps(final) + "\n\n"

        headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
        return StreamingResponse(gen(), media_type="text/event-stream", headers=headers)

    # Non-streaming path: prefer Agents SDK if available
    text = ""
    used_agents_sdk = False
    try:
        from app.agents.roles import get_agent  # type: ignore
        from agents import Runner  # type: ignore

        agent = get_agent(payload.agent)
        if agent is not None:
            TOOL_TRACE_CVAR.set([])
            # If asked to use multi-agent director, delegate
            if payload.agent.strip().upper() in {"DIRECTOR", "AUTO"}:
                # Seed a multi-step plan prompt to encourage explicit tool orchestration
                planned = (
                    "Plan then act: 1) run_all_specialists_parallel on the user input; "
                    "2) review results; 3) call alchemist or analyst if necessary; 4) synthesize final answer.\n\n"
                    f"User: {payload.query}"
                )
                result = Runner.run_sync(agent, planned, max_turns=40)
            else:
                result = Runner.run_sync(agent, payload.query, max_turns=12)
            text = getattr(result, "final_output", None) or ""
            used_agents_sdk = True
    except Exception:
        used_agents_sdk = False

    if not used_agents_sdk:
        from app.main import build_system_prompt  # lazy import
        resp = client.responses.create(
            model="gpt-4o-mini",
            instructions=build_system_prompt(payload.agent),
            input=f"User: {payload.query}\nAssistant:",
            temperature=0.2,
            max_output_tokens=1000,
        )
        text = getattr(resp, "output_text", "") or ""
    conn3 = _connect()
    cur3 = conn3.cursor()
    cur3.execute(
        "UPDATE tasks SET status=?, answer_markdown=?, updated_at=? WHERE id=?",
        ("succeeded", text, _now_iso(), task_id),
    )
    cur3.execute(
        "INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)",
        (task_id, "AI", text, _now_iso()),
    )
    conn3.commit()
    conn3.close()
    _insert_evidence_for_task(task_id, text)
    return JSONResponse({
        "task_id": task_id,
        "status": "succeeded",
        "answer_markdown": text,
        "citations": extract_citations(text or ""),
        "tool_trace": TOOL_TRACE_CVAR.get() or [],
    }, status_code=202)


@router.get("/v1/tasks/{task_id}")
async def get_task(task_id: str):
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT * FROM tasks WHERE id=?", (task_id,))
    row = cur.fetchone()
    if not row:
        conn.close()
        return JSONResponse({"error": "Not found"}, status_code=404)
    cur.execute("SELECT author, content, created_at FROM messages WHERE task_id=? ORDER BY id", (task_id,))
    msgs = [dict(r) for r in cur.fetchall()]
    conn.close()
    return {
        "task_id": row["id"],
        "agent": row["agent"],
        "status": row["status"],
        "answer_markdown": row["answer_markdown"],
        "citations": extract_citations((row["answer_markdown"] or "")),
        "tool_trace": [],
        "messages": msgs,
    }


class ContinueTaskRequest(BaseModel):
    message: str
    stream: Optional[bool] = False


@router.post("/v1/tasks/{task_id}/continue")
async def continue_task(task_id: str, req: Request):
    body = await req.json()
    try:
        payload = ContinueTaskRequest(**body)
    except Exception:
        return JSONResponse({"error": "Invalid request"}, status_code=400)

    auth = req.headers.get("authorization") or ""
    bearer = auth.split(" ", 1)[1] if auth.lower().startswith("bearer ") and len(auth.split(" ", 1)) > 1 else None
    api_key = bearer or os.getenv("OPENAI_API_KEY", "")
    if not api_key:
        return JSONResponse({"error": "Missing OpenAI API key"}, status_code=401)

    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT * FROM tasks WHERE id=?", (task_id,))
    row = cur.fetchone()
    if not row:
        conn.close()
        return JSONResponse({"error": "Not found"}, status_code=404)
    cur.execute("INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)", (task_id, "User", payload.message, _now_iso()))
    conn.commit()
    # gather transcript
    cur.execute("SELECT author, content FROM messages WHERE task_id=? ORDER BY id", (task_id,))
    convo = cur.fetchall()
    conn.close()

    transcript = []
    for r in convo:
        role = "User" if r["author"] == "User" else "Assistant"
        transcript.append(f"{role}: {r['content']}")
    transcript.append("Assistant:")
    input_text = "\n".join(transcript)

    client = OpenAI(api_key=api_key)

    if payload.stream:
        def gen():
            buffer = ""
            yield "event: open\n\n"
            with client.responses.stream(
                model="gpt-4o-mini",
                input=input_text,
                temperature=0.2,
                max_output_tokens=1000,
            ) as stream:
                for event in stream:
                    if getattr(event, "type", "") == "response.output_text.delta":
                        delta = getattr(event, "delta", "") or ""
                        if delta:
                            buffer += delta
                            yield "data: " + json.dumps({"delta": delta}) + "\n\n"
                final_resp = stream.get_final_response()
                text = getattr(final_resp, "output_text", None) or buffer

            conn2 = _connect()
            cur2 = conn2.cursor()
            cur2.execute(
                "UPDATE tasks SET status=?, answer_markdown=?, updated_at=? WHERE id=?",
                ("succeeded", text, _now_iso(), task_id),
            )
            cur2.execute(
                "INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)",
                (task_id, "AI", text, _now_iso()),
            )
            conn2.commit()
            conn2.close()

            yield "data: " + json.dumps({"done": True, "task_id": task_id}) + "\n\n"

        headers = {"Cache-Control": "no-cache", "X-Accel-Buffering": "no"}
        return StreamingResponse(gen(), media_type="text/event-stream", headers=headers)

    # non-stream
    resp = client.responses.create(
        model="gpt-4o-mini",
        input=input_text,
        temperature=0.2,
        max_output_tokens=1000,
    )
    text = getattr(resp, "output_text", "") or ""
    conn3 = _connect()
    cur3 = conn3.cursor()
    cur3.execute(
        "UPDATE tasks SET status=?, answer_markdown=?, updated_at=? WHERE id=?",
        ("succeeded", text, _now_iso(), task_id),
    )
    cur3.execute(
        "INSERT INTO messages (task_id, author, content, created_at) VALUES (?, ?, ?, ?)",
        (task_id, "AI", text, _now_iso()),
    )
    conn3.commit()
    conn3.close()
    return {"task_id": task_id, "status": "succeeded", "answer_markdown": text}
</file>

<file path="apps/backend/app/routes/workflows.py">
from fastapi import APIRouter, Header
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import uuid

router = APIRouter()


class WorkflowRequest(BaseModel):
    name: str
    nodes: list[dict]
    edges: list[dict]


@router.post("/v1/workflows")
async def run_workflow(payload: WorkflowRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    return {"workflowId": str(uuid.uuid4()), "status": "queued", "note": "Phase 0 stub"}
</file>

<file path="apps/backend/app/services/chem.py">
from typing import List
from fastapi import APIRouter, Header
from pydantic import BaseModel
from fastapi.responses import JSONResponse

router = APIRouter()


class Candidate(BaseModel):
    smiles: str
    score: float
    rationale: str


class DesignRequest(BaseModel):
    constraints: dict | None = None
    n: int = 3


@router.post("/chem/design")
async def chem_design(payload: DesignRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    cands: List[Candidate] = [
        Candidate(smiles="CCO", score=0.42, rationale="Stub candidate: small alcohol").model_dump(),
        Candidate(smiles="c1ccccc1", score=0.35, rationale="Stub candidate: benzene core").model_dump(),
        Candidate(smiles="CC(=O)O", score=0.28, rationale="Stub candidate: acetate motif").model_dump(),
    ]
    n = max(1, min(payload.n, 5))
    return {"candidates": cands[:n], "disclaimer": "Stub output for Phase 0"}
</file>

<file path="apps/backend/app/services/docs.py">
import uuid
from fastapi import APIRouter, Header, Query
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import httpx
import xml.etree.ElementTree as ET

router = APIRouter()


class IngestRequest(BaseModel):
    doi: str | None = None
    url: str | None = None


@router.post("/docs/ingest")
async def docs_ingest(payload: IngestRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    return {"docId": str(uuid.uuid4()), "status": "queued", "note": "Stub ingestion for Phase 0"}


@router.post("/v1/documents:ingest")
async def v1_documents_ingest(payload: IngestRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    # Reuse the same stub behavior for Phase 0
    return {"docId": str(uuid.uuid4()), "status": "queued", "note": "Stub ingestion for Phase 0"}


@router.get("/docs/crossref")
async def docs_crossref(q: str = Query(..., min_length=2), rows: int = 5, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    url = "https://api.crossref.org/works"
    params = {"query": q, "rows": max(1, min(rows, 20))}
    try:
        async with httpx.AsyncClient(timeout=20) as client:
            resp = await client.get(url, params=params)
            resp.raise_for_status()
            data = resp.json()
            items = data.get("message", {}).get("items", [])
            results = [{
                "title": (i.get("title") or [""])[0],
                "doi": i.get("DOI"),
                "url": i.get("URL"),
                "issued": i.get("issued"),
                "author": i.get("author"),
            } for i in items]
            return {"results": results}
    except Exception as e:
        return {"error": str(e), "results": []}


@router.get("/docs/pubmed")
async def docs_pubmed(q: str = Query(..., min_length=2), retmax: int = 10, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    esearch = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
    esummary = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
    try:
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(esearch, params={"db": "pubmed", "retmode": "json", "term": q, "retmax": max(1, min(retmax, 50))})
            r.raise_for_status()
            ids = (r.json().get("esearchresult", {}).get("idlist", []))
            if not ids:
                return {"results": []}
            s = await client.get(esummary, params={"db": "pubmed", "retmode": "json", "id": ",".join(ids)})
            s.raise_for_status()
            res = s.json().get("result", {})
            uids = res.get("uids", [])
            items = []
            for uid in uids:
                it = res.get(uid) or {}
                items.append({
                    "uid": uid,
                    "title": it.get("title"),
                    "pubdate": it.get("pubdate"),
                    "doi": (it.get("elocationid") or "").split("doi:")[-1].strip() if "doi:" in (it.get("elocationid") or "") else None,
                })
            return {"results": items}
    except Exception as e:
        return {"error": str(e), "results": []}


@router.get("/docs/unpaywall")
async def docs_unpaywall(doi: str, email: str = Query("dev@example.com"), authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    try:
        url = f"https://api.unpaywall.org/v2/{doi}"
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(url, params={"email": email})
            r.raise_for_status()
            return r.json()
    except Exception as e:
        return {"error": str(e)}


# fix arXiv: follow https
@router.get("/docs/arxiv")
async def docs_arxiv(q: str = Query(..., min_length=2), max_results: int = 5, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    import xml.etree.ElementTree as ET
    url = "https://export.arxiv.org/api/query"
    params = {"search_query": f"all:{q}", "start": 0, "max_results": max(1, min(max_results, 20))}
    try:
        async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:
            resp = await client.get(url, params=params)
            resp.raise_for_status()
            root = ET.fromstring(resp.text)
            ns = {"a": "http://www.w3.org/2005/Atom"}
            entries = []
            for entry in root.findall("a:entry", ns):
                title = (entry.find("a:title", ns).text or "").strip()
                links = [l.attrib.get("href") for l in entry.findall("a:link", ns)]
                idv = (entry.find("a:id", ns).text or "").strip()
                summary = (entry.find("a:summary", ns).text or "").strip()
                entries.append({"title": title, "id": idv, "links": links, "summary": summary})
            return {"results": entries}
    except Exception as e:
        return {"error": str(e), "results": []}
</file>

<file path="apps/backend/app/services/models.py">
import os
from typing import Optional
import requests
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel

router = APIRouter()

HF_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN", "")
HF_API = "https://api-inference.huggingface.co/pipeline/feature-extraction/facebook/esm2_t33_650M_UR50D"


class ESM2Request(BaseModel):
    sequence: str


@router.post("/models/esm2/embeddings")
def esm2_embeddings(body: ESM2Request):
    if not HF_TOKEN:
        raise HTTPException(500, "Missing HUGGINGFACE_API_TOKEN")
    seq = body.sequence.strip().upper()
    if not seq:
        raise HTTPException(400, "Empty sequence")
    # Validate simple protein alphabet
    for ch in seq:
        if ch not in "ACDEFGHIKLMNPQRSTVWYBXZJUO*":
            raise HTTPException(400, f"Invalid residue: {ch}")
    headers = {"Authorization": f"Bearer {HF_TOKEN}"}
    try:
        r = requests.post(HF_API, headers=headers, json={"inputs": seq}, timeout=60)
        if r.status_code != 200:
            raise HTTPException(502, f"HF error: {r.text}")
        arr = r.json()
        # arr: [tokens x hidden] — compute mean embedding
        if not isinstance(arr, list) or not arr:
            raise HTTPException(502, "Invalid HF response")
        # Some deployments wrap as [[...]]; flatten once
        if isinstance(arr[0], list) and isinstance(arr[0][0], list):
            arr = arr[0]
        hidden = len(arr[0])
        means = [sum(row[i] for row in arr) / len(arr) for i in range(hidden)]
        return {"length": len(seq), "hidden": hidden, "mean": means[:64]}  # preview first 64 dims
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(500, "Embedding failed")
</file>

<file path="apps/backend/app/services/rag.py">
from typing import List, Dict, Tuple
from fastapi import APIRouter, Header
from pydantic import BaseModel
from fastapi.responses import JSONResponse
import os
import sqlite3
import time
import math
import json
import re

try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # type: ignore

router = APIRouter()

DB_PATH = os.getenv("RUNIX_TASKS_DB", os.path.join(os.path.dirname(__file__), "..", "tasks.db"))


def _connect():
    path = os.path.abspath(DB_PATH)
    os.makedirs(os.path.dirname(path), exist_ok=True)
    conn = sqlite3.connect(path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    return conn


def _init_db():
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS documents (
            id TEXT PRIMARY KEY,
            doi TEXT,
            url TEXT,
            title TEXT,
            abstract TEXT,
            source_type TEXT,
            created_at TEXT
        )
        """
    )
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS passages (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            doc_id TEXT NOT NULL,
            section TEXT,
            span_start INTEGER,
            span_end INTEGER,
            text TEXT
        )
        """
    )
    cur.execute(
        """
        CREATE TABLE IF NOT EXISTS embeddings (
            passage_id INTEGER PRIMARY KEY,
            model TEXT,
            vector TEXT
        )
        """
    )
    conn.commit()
    conn.close()


_init_db()


class Passage(BaseModel):
    source: str
    text: str
    span: List[int]


TOKEN_RE = re.compile(r"[A-Za-z0-9_]+")


def _tokenize(text: str) -> List[str]:
    return [t.lower() for t in TOKEN_RE.findall(text or "")]


def _compute_bm25(corpus: List[List[str]], query_terms: List[str], k1: float = 1.5, b: float = 0.75) -> List[float]:
    N = len(corpus)
    avgdl = sum(len(doc) for doc in corpus) / (N or 1)
    # document frequencies
    df: Dict[str, int] = {}
    for doc in corpus:
        seen = set(doc)
        for term in seen:
            df[term] = df.get(term, 0) + 1
    scores: List[float] = []
    for doc in corpus:
        score = 0.0
        dl = len(doc)
        tf: Dict[str, int] = {}
        for t in doc:
            tf[t] = tf.get(t, 0) + 1
        for q in query_terms:
            if q not in df:
                continue
            idf = math.log(1 + (N - df[q] + 0.5) / (df[q] + 0.5))
            freq = tf.get(q, 0)
            denom = freq + k1 * (1 - b + b * (dl / (avgdl or 1)))
            term_score = idf * ((freq * (k1 + 1)) / (denom or 1))
            score += term_score
        scores.append(score)
    return scores


def _cosine(a: List[float], b: List[float]) -> float:
    if not a or not b or len(a) != len(b):
        return 0.0
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    if na == 0 or nb == 0:
        return 0.0
    return dot / (na * nb)


def _get_embedding(text: str, model: str = "text-embedding-3-large") -> List[float] | None:
    api_key = os.getenv("OPENAI_API_KEY", "")
    if not api_key or OpenAI is None:
        return None
    try:
        client = OpenAI(api_key=api_key)
        resp = client.embeddings.create(model=model, input=text)
        return resp.data[0].embedding  # type: ignore
    except Exception:
        return None


class IndexRequest(BaseModel):
    doc_id: str
    title: str | None = None
    doi: str | None = None
    url: str | None = None
    text: str
    section: str | None = None


@router.post("/rag/index")
async def rag_index(payload: IndexRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    text = (payload.text or "").strip()
    if not text:
        return JSONResponse({"error": "Missing text"}, status_code=400)
    # Upsert document
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "INSERT OR REPLACE INTO documents (id, doi, url, title, abstract, source_type, created_at) VALUES (?, ?, ?, ?, ?, ?, ?)",
        (
            payload.doc_id,
            payload.doi,
            payload.url,
            payload.title,
            None,
            "manual",
            time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        ),
    )
    # Split into simple passages (paragraphs)
    spans: List[Tuple[int, int, str]] = []
    offset = 0
    for para in re.split(r"\n\n+", text):
        p = para.strip()
        if not p:
            offset += len(para) + 2
            continue
        start = offset
        end = offset + len(p)
        spans.append((start, end, p))
        offset = end + 2
    # Insert passages + embeddings
    inserted = 0
    for (s, e, ptxt) in spans:
        cur.execute(
            "INSERT INTO passages (doc_id, section, span_start, span_end, text) VALUES (?, ?, ?, ?, ?)",
            (payload.doc_id, payload.section or "body", s, e, ptxt),
        )
        pid = cur.lastrowid
        emb = _get_embedding(ptxt)
        if emb is not None:
            cur.execute(
                "INSERT OR REPLACE INTO embeddings (passage_id, model, vector) VALUES (?, ?, ?)",
                (pid, "text-embedding-3-large", json.dumps(emb)),
            )
        inserted += 1
    conn.commit()
    conn.close()
    return {"indexed_passages": inserted}


class ExpandRequest(BaseModel):
    q: str
    n: int = 3


@router.post("/rag/expand")
async def rag_expand(payload: ExpandRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    q = payload.q.strip()
    rewrites = [
        f"{q} recent systematic reviews",
        f"{q} randomized controlled trials 2020..now",
        f"{q} mechanisms and biomarkers",
    ]
    return {"queries": rewrites[: max(1, min(payload.n, 5))]}


@router.get("/rag/search")
async def rag_search(q: str, k: int = 3, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id, doc_id, text FROM passages")
    rows = cur.fetchall()
    conn.close()
    passages = [(r["id"], r["doc_id"], r["text"]) for r in rows]
    tokens = [_tokenize(p[2]) for p in passages]
    q_terms = _tokenize(q)
    bm25_scores = _compute_bm25(tokens, q_terms) if passages else []
    # Prepare initial ranking
    scored = list(zip(passages, bm25_scores))
    scored.sort(key=lambda x: x[1], reverse=True)
    top = scored[: max(10, k)]
    # Optional embedding rerank for the top window
    q_emb = _get_embedding(q)
    if q_emb is not None:
        reranked: List[Tuple[Tuple[int, str, str], float]] = []
        for (pid, did, text), bm in top:
            # fetch embedding if available
            conn2 = _connect()
            cur2 = conn2.cursor()
            cur2.execute("SELECT vector FROM embeddings WHERE passage_id=?", (pid,))
            er = cur2.fetchone()
            conn2.close()
            if er:
                try:
                    vec = json.loads(er["vector"])
                    sim = _cosine(q_emb, vec)
                except Exception:
                    sim = 0.0
            else:
                sim = 0.0
            # hybrid score: 0.7 bm25 + 0.3 sim
            score = 0.7 * bm + 0.3 * sim
            reranked.append(((pid, did, text), score))
        reranked.sort(key=lambda x: x[1], reverse=True)
        top = reranked
    # Format output and cluster by doc_id
    results: List[Dict] = []
    cluster_map: Dict[str, List[int]] = {}
    for (pid, did, text), sc in top[:k]:
        results.append(Passage(source=str(did), text=text, span=[0, max(0, min(len(text), 200))]).model_dump())
        cluster_map.setdefault(str(did), []).append(pid)
    clusters = [{"doc_id": d, "passages": pids} for d, pids in cluster_map.items()]
    return {"passages": results, "clusters": clusters}


class IngestSourceRequest(BaseModel):
    source_type: str  # 'doi' | 'arxiv'
    id: str           # DOI string or arXiv id/url


@router.post("/rag/ingest_from_source")
async def rag_ingest_from_source(payload: IngestSourceRequest, authorization: str | None = Header(default=None)):
    if not authorization:
        return JSONResponse({"error": "Unauthorized"}, status_code=401)
    st = (payload.source_type or '').lower()
    doc_id = payload.id
    title = None
    text = None
    if st == 'doi':
        # fetch Unpaywall metadata (title, abstract if present)
        import httpx
        async with httpx.AsyncClient(timeout=20) as client:
            r = await client.get(f"https://api.unpaywall.org/v2/{doc_id}", params={"email": os.getenv("UNPAYWALL_EMAIL", "dev@example.com")})
            if r.status_code == 200:
                data = r.json()
                title = data.get('title')
                # fallback: combine best OA location title/host
                text = (data.get('oa_locations') or [{}])[0].get('host_type') or ''
    elif st == 'arxiv':
        # fetch arXiv atom entry summary
        import httpx, xml.etree.ElementTree as ET
        async with httpx.AsyncClient(timeout=20, follow_redirects=True) as client:
            url = "https://export.arxiv.org/api/query"
            r = await client.get(url, params={"search_query": f"id:{doc_id}", "start": 0, "max_results": 1})
            if r.status_code == 200:
                root = ET.fromstring(r.text)
                ns = {"a": "http://www.w3.org/2005/Atom"}
                entry = root.find("a:entry", ns)
                if entry is not None:
                    title = (entry.find("a:title", ns).text or "").strip()
                    text = (entry.find("a:summary", ns).text or "").strip()
    else:
        return JSONResponse({"error": "Unsupported source_type"}, status_code=400)

    if not text:
        return JSONResponse({"error": "No text available to index"}, status_code=400)

    # index into passages
    idx = await rag_index(IndexRequest(doc_id=doc_id, title=title, doi=doc_id if st=='doi' else None, url=None, text=text, section='abstract'), authorization=authorization)  # type: ignore
    try:
        return idx  # type: ignore
    except Exception:
        return {"ok": True}
</file>

<file path="apps/backend/app/tools/__init__.py">
from .local import extract_citations, format_markdown_with_refs, chem_calc

__all__ = [
    "extract_citations",
    "format_markdown_with_refs",
    "chem_calc",
]
</file>

<file path="apps/backend/app/tools/local.py">
from typing import List, Dict


def extract_citations(text: str) -> List[Dict]:
    """Stub citation extractor. Returns a few made-up entries if it detects [1], [2]."""
    results: List[Dict] = []
    if "[1]" in text:
        results.append({
            "index": 1,
            "title": "Example Study",
            "doi": "10.1000/example",
            "url": "https://doi.org/10.1000/example",
            "snippet": "An example cited passage."
        })
    if "[2]" in text:
        results.append({
            "index": 2,
            "title": "Another Study",
            "doi": "10.1000/another",
            "url": "https://doi.org/10.1000/another",
            "snippet": "Another example cited passage."
        })
    return results


def format_markdown_with_refs(blocks: List[Dict]) -> str:
    """Render simple markdown with numeric references at the end."""
    body_parts: List[str] = []
    refs: List[Dict] = []
    for b in blocks:
        if b.get("type") == "paragraph":
            body_parts.append(b.get("text", ""))
        if b.get("type") == "citation":
            refs.append(b.get("ref", {}))
    md = "\n\n".join(body_parts)
    if refs:
        md += "\n\nReferences:\n"
        for i, r in enumerate(refs, start=1):
            md += f"[{i}] {r.get('title','Untitled')} — {r.get('doi','')}\n"
    return md


def chem_calc(smiles: str) -> Dict:
    """Dummy chemistry calculator. Returns pretend properties."""
    return {
        "smiles": smiles,
        "mw": 42.0,
        "hbd": 1,
        "hba": 1,
        "logp": 1.23,
        "note": "Stub properties for Phase 0",
    }
</file>

<file path="apps/backend/app/cli.py">
import argparse
import json
import sys
from typing import Dict, Optional

import httpx


def _build_headers(api_key: Optional[str]) -> Dict[str, str]:
    headers: Dict[str, str] = {"content-type": "application/json"}
    if api_key:
        headers["authorization"] = f"Bearer {api_key}"
    return headers


def cmd_list_agents(base_url: str, api_key: Optional[str]) -> int:
    with httpx.Client(timeout=20) as client:
        resp = client.get(f"{base_url}/v1/agents", headers=_build_headers(api_key))
        resp.raise_for_status()
        data = resp.json()
        agents = data.get("agents", [])
        print("Agents:")
        for name in agents:
            print(f"- {name}")
    return 0


def _print_streaming_response(resp: httpx.Response) -> None:
    buffer: str = ""
    for line in resp.iter_lines():
        if not line:
            continue
        try:
            text = line.decode("utf-8") if isinstance(line, (bytes, bytearray)) else str(line)
        except Exception:
            continue
        if text.startswith("data: "):
            payload = text[len("data: "):]
            try:
                obj = json.loads(payload)
            except Exception:
                continue
            # stream deltas
            if "delta" in obj:
                delta = obj["delta"]
                buffer += delta
                sys.stdout.write(delta)
                sys.stdout.flush()
            # final
            if obj.get("done"):
                print()
                return


def cmd_create_task(base_url: str, api_key: Optional[str], agent: str, query: str, stream: bool) -> int:
    body = {"agent": agent, "query": query, "stream": stream}
    with httpx.Client(timeout=None) as client:
        if stream:
            with client.stream("POST", f"{base_url}/v1/tasks", headers=_build_headers(api_key), json=body) as resp:
                if resp.status_code >= 400:
                    print(f"Error {resp.status_code}: {resp.text}")
                    return 1
                print(f"[streaming] {agent}: {query}\n")
                _print_streaming_response(resp)
                return 0
        else:
            resp = client.post(f"{base_url}/v1/tasks", headers=_build_headers(api_key), json=body)
            if resp.status_code >= 400:
                print(f"Error {resp.status_code}: {resp.text}")
                return 1
            data = resp.json()
            task_id = data.get("task_id")
            print(json.dumps(data, indent=2))
            print(f"\nTask ID: {task_id}")
    return 0


def cmd_get_task(base_url: str, api_key: Optional[str], task_id: str) -> int:
    with httpx.Client(timeout=20) as client:
        resp = client.get(f"{base_url}/v1/tasks/{task_id}", headers=_build_headers(api_key))
        if resp.status_code >= 400:
            print(f"Error {resp.status_code}: {resp.text}")
            return 1
        print(json.dumps(resp.json(), indent=2))
    return 0


def cmd_continue_task(base_url: str, api_key: Optional[str], task_id: str, message: str, stream: bool) -> int:
    body = {"message": message, "stream": stream}
    with httpx.Client(timeout=None) as client:
        if stream:
            with client.stream("POST", f"{base_url}/v1/tasks/{task_id}/continue", headers=_build_headers(api_key), json=body) as resp:
                if resp.status_code >= 400:
                    print(f"Error {resp.status_code}: {resp.text}")
                    return 1
                print(f"[streaming] continue {task_id}: {message}\n")
                _print_streaming_response(resp)
                return 0
        else:
            resp = client.post(f"{base_url}/v1/tasks/{task_id}/continue", headers=_build_headers(api_key), json=body)
            if resp.status_code >= 400:
                print(f"Error {resp.status_code}: {resp.text}")
                return 1
            print(json.dumps(resp.json(), indent=2))
    return 0


def main() -> int:
    parent = argparse.ArgumentParser(add_help=False)
    parent.add_argument("--base-url", default="http://localhost:8787", help="Backend base URL")
    parent.add_argument("--api-key", default=None, help="Bearer token for backend (optional)")

    parser = argparse.ArgumentParser(prog="runix-agents", description="CLI to test backend agents")
    sub = parser.add_subparsers(dest="cmd", required=True)

    sub.add_parser("list", parents=[parent], help="List available agents")

    s_task = sub.add_parser("task", parents=[parent], help="Create a new task")
    s_task.add_argument("agent", help="Agent name, e.g. SCOUT/SCHOLAR/ARCHIVIST/ALCHEMIST/ANALYST")
    s_task.add_argument("query", help="User query text")
    s_task.add_argument("--stream", action="store_true", help="Stream the response")

    s_get = sub.add_parser("get", parents=[parent], help="Get a task by id")
    s_get.add_argument("task_id", help="Task id")

    s_cont = sub.add_parser("continue", parents=[parent], help="Continue a task with a new user message")
    s_cont.add_argument("task_id", help="Task id")
    s_cont.add_argument("message", help="Message to append")
    s_cont.add_argument("--stream", action="store_true", help="Stream the response")

    # Accept global flags before or after the subcommand
    try:
        args = parser.parse_intermixed_args()  # type: ignore[attr-defined]
    except Exception:
        args = parser.parse_args()

    base_url: str = args.base_url.rstrip("/")
    api_key: Optional[str] = args.api_key

    if args.cmd == "list":
        return cmd_list_agents(base_url, api_key)
    if args.cmd == "task":
        return cmd_create_task(base_url, api_key, args.agent, args.query, args.stream)
    if args.cmd == "get":
        return cmd_get_task(base_url, api_key, args.task_id)
    if args.cmd == "continue":
        return cmd_continue_task(base_url, api_key, args.task_id, args.message, args.stream)

    parser.print_help()
    return 2


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="apps/backend/app/main.py">
import os
from typing import List, Literal, Optional

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv
from openai import OpenAI
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
import logging
import json as _json

# Local imports
try:
    from app.routes.tasks import router as tasks_router
    from app.routes.agents import router as agents_router
    from app.services.rag import router as rag_router
    from app.services.chem import router as chem_router
    from app.services.docs import router as docs_router
    from app.routes.evidence import router as evidence_router
    from app.routes.repo import router as repo_router
    from app.routes.streams import router as streams_router
    from app.routes.workflows import router as workflows_router
    from app.services.models import router as models_router
except Exception:  # pragma: no cover - allow running as module
    from .routes.tasks import router as tasks_router  # type: ignore
    from .routes.agents import router as agents_router  # type: ignore
    from .services.rag import router as rag_router  # type: ignore
    from .services.chem import router as chem_router  # type: ignore
    from .services.docs import router as docs_router  # type: ignore
    from .routes.evidence import router as evidence_router  # type: ignore
    from .routes.repo import router as repo_router  # type: ignore
    from .routes.streams import router as streams_router  # type: ignore
    from .routes.workflows import router as workflows_router  # type: ignore
    from .services.models import router as models_router  # type: ignore

# Load environment variables from multiple potential .env locations
load_dotenv()  # current working directory
_here = os.path.dirname(__file__)
_app_env = os.path.abspath(os.path.join(_here, ".env"))
_backend_env = os.path.abspath(os.path.join(_here, "..", ".env"))
for _p in (_app_env, _backend_env):
    if os.path.exists(_p):
        load_dotenv(_p, override=False)

DEFAULT_OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

app = FastAPI(title="Runix Backend", version="0.1.0")
# JSON logging
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger("runix")


origins = os.getenv("CORS_ORIGINS", "http://localhost:3000").split(",")
app.add_middleware(
    CORSMiddleware,
    allow_origins=[o.strip() for o in origins if o.strip()],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class Message(BaseModel):
    author: Literal["User", "AI"]
    content: str


class ChatRequest(BaseModel):
    messages: List[Message]
    agent: Optional[Literal["crow", "falcon", "owl", "phoenix"]] = "crow"
    stream: Optional[bool] = False
    goal: Optional[str] = None
    mode: Optional[Literal["qa", "review", "novelty", "planner"]] = None
    output: Optional[Literal["bullets", "memo", "plan", "json"]] = None
    temperature: Optional[float] = None
    max_output_tokens: Optional[int] = None


def build_system_prompt(agent: str, goal: Optional[str] = None, mode: Optional[str] = None, output: Optional[str] = None) -> str:
    base = (
        "You are Runix AI, a helpful research assistant for scientific discovery. "
        "Prefer verifiable claims with citations and high-level reasoning summaries (no hidden steps). "
        "Always be precise and note uncertainties."
    )
    if agent == "falcon":
        base += " Focus on deep literature survey and meta-analysis across full texts."
    if agent == "owl":
        base += " Perform novelty checks and prior-art sweeps: 'Has anyone done X?'."
    if agent == "phoenix":
        base += " Act as a ChemCrow-style planner. Emphasize verification and uncertainty notes."

    if mode == "review":
        base += " Structure as: Scope, Key Findings, Methods Notes, Gaps, References."
    elif mode == "novelty":
        base += " Structure as: Query framing, Known Work, Potential Gaps, Confidence, References."
    elif mode == "planner":
        base += " Structure as: Objective, Approach, Steps, Reagents/Tools, Risks, Alternatives."

    if output == "bullets":
        base += " Use compact bullet lists."
    elif output == "memo":
        base += " Write a concise research memo."
    elif output == "plan":
        base += " Present a stepwise plan with materials."
    elif output == "json":
        base += " Return JSON with fields: summary, steps[], citations[]."

    if goal:
        base += f" Current goal: {goal.strip()[:500]}"

    return base


def to_transcript(payload: ChatRequest) -> str:
    # Convert structured messages to a plain transcript for Responses API input
    lines: list[str] = []
    for m in payload.messages:
        role = "User" if m.author == "User" else "Assistant"
        lines.append(f"{role}: {m.content}")
    lines.append("Assistant:")
    return "\n".join(lines)


@app.post("/chat")
async def chat(req: Request):
    # Prefer bearer from Authorization header; fallback to env
    auth = req.headers.get("authorization") or ""
    bearer = auth.split(" ", 1)[1] if auth.lower().startswith("bearer ") and len(auth.split(" ", 1)) > 1 else None
    openai_key = bearer or DEFAULT_OPENAI_API_KEY
    if not openai_key:
        return JSONResponse({"error": "Missing OpenAI API key"}, status_code=401)

    body = await req.json()
    try:
        payload = ChatRequest(**body)
    except Exception:
        return JSONResponse({"error": "Invalid request"}, status_code=400)

    client = OpenAI(api_key=openai_key)

    if payload.stream:
        def gen():
            system = build_system_prompt(payload.agent or "crow", payload.goal, payload.mode, payload.output)
            yield "event: open\n\n"
            buffer = ""
            with client.responses.stream(
                model="gpt-4o-mini",
                instructions=system,
                input=to_transcript(payload),
                temperature=payload.temperature if payload.temperature is not None else 0.7,
                max_output_tokens=payload.max_output_tokens if payload.max_output_tokens is not None else 1000,
            ) as stream:
                for event in stream:
                    # Text deltas
                    et = getattr(event, "type", "")
                    if et == "response.output_text.delta":
                        delta = getattr(event, "delta", "") or ""
                        if delta:
                            buffer += delta
                            yield "data: " + _json.dumps({"delta": delta}) + "\n\n"
                    # Log non-text events briefly
                    if et and et != "response.output_text.delta":
                        logger.info(_json.dumps({"event": et}))
                    # You may also inspect other event types if needed
                # Final assembled response
                final_resp = stream.get_final_response()
                content_text = getattr(final_resp, "output_text", None) or buffer
                final = {"done": True, "message": {"id": 0, "author": "AI", "content": content_text}}
                yield "data: " + _json.dumps(final) + "\n\n"

        return StreamingResponse(gen(), media_type="text/event-stream")

    # Non-stream path
    resp = client.responses.create(
        model="gpt-4o-mini",
        instructions=build_system_prompt(payload.agent or "crow", payload.goal, payload.mode, payload.output),
        input=to_transcript(payload),
        temperature=payload.temperature if payload.temperature is not None else 0.7,
        max_output_tokens=payload.max_output_tokens if payload.max_output_tokens is not None else 1000,
    )
    content = getattr(resp, "output_text", None)
    if not content:
        # Fallback: try to extract first text block
        try:
            content = resp.output[0].content[0].text["value"]  # type: ignore
        except Exception:
            content = ""
    return {"message": {"id": 0, "author": "AI", "content": content}}


# Healthcheck
@app.get("/health")
async def health():
    return {"ok": True}


# Routers for Phase 0 services and tasks API
app.include_router(tasks_router, prefix="")
app.include_router(agents_router, prefix="")
app.include_router(rag_router, prefix="/services")
app.include_router(chem_router, prefix="/services")
app.include_router(docs_router, prefix="/services")
app.include_router(models_router, prefix="/services")
app.include_router(evidence_router, prefix="")
app.include_router(streams_router, prefix="")
app.include_router(workflows_router, prefix="")
app.include_router(repo_router, prefix="")


# Serve OpenAPI action specs statically
infra_actions_dir = os.path.join(os.path.dirname(__file__), "infra", "actions")
if os.path.isdir(os.path.abspath(infra_actions_dir)):
    app.mount(
        "/infra/actions",
        StaticFiles(directory=os.path.abspath(infra_actions_dir)),
        name="infra-actions",
    )


def main():
    import uvicorn

    uvicorn.run("app.main:app", host="0.0.0.0", port=8787, reload=True)
</file>

<file path="apps/backend/Dockerfile">
FROM python:3.11-slim

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=on

WORKDIR /app

# System deps
RUN apt-get update && apt-get install -y --no-install-recommends build-essential && rm -rf /var/lib/apt/lists/*

COPY pyproject.toml uv.lock ./
RUN pip install --no-cache-dir uv && \
    uv pip install --system -r <(uv pip compile --generate-hashes pyproject.toml)

COPY app ./app

EXPOSE 8787

CMD ["python", "-m", "app.main"]
</file>

<file path="apps/backend/pyproject.toml">
[project]
name = "runix-backend"
version = "0.1.0"
description = "Runix scientific agents backend (FastAPI + OpenAI)"
requires-python = ">=3.10"
readme = "README.md"
authors = [
  { name = "Runix", email = "dev@runix.local" }
]

dependencies = [
  "fastapi>=0.110.0",
  "uvicorn>=0.30.0",
  "pydantic>=2.6.0",
  "python-dotenv>=1.0.1",
  "openai>=1.40.0",
  "httpx>=0.27.0",
  "tenacity>=8.2.3",
  "openai-agents>=0.2.0",
  "mcp>=1.2.0",
]

[project.optional-dependencies]
dev = [
  "ruff>=0.5.0",
  "pytest>=8.2.0",
  "httpx[client]>=0.27.0",
]

[project.scripts]
runix-backend = "app.main:main"
runix-agents = "app.cli:main"

[tool.uv]
index-url = "https://pypi.org/simple"

[tool.ruff]
line-length = 100
target-version = "py310"
</file>

</files>
